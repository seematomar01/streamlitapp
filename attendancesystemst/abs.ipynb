{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5ef885a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import queue\n",
    "import threading\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from typing import List, NamedTuple, Optional\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import  datetime\n",
    "import av\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pydub\n",
    "import streamlit as st\n",
    "from aiortc.contrib.media import MediaPlayer\n",
    "from streamlit_webrtc import (\n",
    "    RTCConfiguration,\n",
    "    WebRtcMode,\n",
    "    WebRtcStreamerContext,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "##\n",
    "import face_recognition as face_rec\n",
    "import cv2\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8bae4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'employee images'\n",
    "employeeImg = []\n",
    "employeeName = []\n",
    "myList = os.listdir(path)\n",
    "filename = 'click'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "51f97bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img, size) :\n",
    "    width = int(img.shape[1]*size)\n",
    "    height = int(img.shape[0] * size)\n",
    "    dimension = (width, height)\n",
    "    return cv2.resize(img, dimension, interpolation= cv2.INTER_AREA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d32e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEncoding(images) :\n",
    "    imgEncodings = []\n",
    "    for img in images :\n",
    "        img = resize(img, 0.50)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        encodeimg = face_rec.face_encodings(img)[0]\n",
    "        imgEncodings.append(encodeimg)\n",
    "    return imgEncodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "52063c8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m EncodeList \u001b[38;5;241m=\u001b[39m findEncoding(employeeImg)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m HERE \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m     20\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "def MarkAttendence(name):\n",
    "    with open('attendence.csv', 'r+') as f:\n",
    "        myDatalist =  f.readlines()\n",
    "        nameList = []\n",
    "        for line in myDatalist :\n",
    "            entry = line.split(',')\n",
    "            nameList.append(entry[0])\n",
    "        if name not in nameList:\n",
    "            now = datetime.now()\n",
    "            timestr = now.strftime('%H:%M')\n",
    "            f.writelines(f'\\n{name}, {timestr}')\n",
    "            statment = str('welcome to seasia' + name)\n",
    "for cl in myList :\n",
    "    curimg = cv2.imread(f'{path}/{cl}')\n",
    "    employeeImg.append(curimg)\n",
    "    employeeName.append(os.path.splitext(cl)[0])\n",
    "EncodeList = findEncoding(employeeImg)\n",
    "\n",
    "HERE = Path(__file__).parent\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28030ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, download_to: Path, expected_size=None):\n",
    "    # Don't download the file twice.\n",
    "    # (If possible, verify the download using the file length.)\n",
    "    if download_to.exists():\n",
    "        if expected_size:\n",
    "            if download_to.stat().st_size == expected_size:\n",
    "                return\n",
    "        else:\n",
    "            st.info(f\"{url} is already downloaded.\")\n",
    "            if not st.button(\"Download again?\"):\n",
    "                return\n",
    "    download_to.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # These are handles to two visual elements to animate.\n",
    "    weights_warning, progress_bar = None, None\n",
    "    try:\n",
    "        weights_warning = st.warning(\"Downloading %s...\" % url)\n",
    "        progress_bar = st.progress(0)\n",
    "        with open(download_to, \"wb\") as output_file:\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                length = int(response.info()[\"Content-Length\"])\n",
    "                counter = 0.0\n",
    "                MEGABYTES = 2.0 ** 20.0\n",
    "                while True:\n",
    "                    data = response.read(8192)\n",
    "                    if not data:\n",
    "                        break\n",
    "                    counter += len(data)\n",
    "                    output_file.write(data)\n",
    "                    # We perform animation by overwriting the elements.\n",
    "                    weights_warning.warning(\n",
    "                        \"Downloading %s... (%6.2f/%6.2f MB)\"\n",
    "                        % (url, counter / MEGABYTES, length / MEGABYTES)\n",
    "                    )\n",
    "                    progress_bar.progress(min(counter / length, 1.0))\n",
    "    # Finally, we remove these visual elements by calling .empty().\n",
    "    finally:\n",
    "        if weights_warning is not None:\n",
    "            weights_warning.empty()\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.empty()\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.header(\"Attendance_system\")\n",
    "    takepic()\n",
    "   \n",
    "    pages = {\n",
    "        \"simple_stream\": app_video_filters,  # noqa: E501\n",
    "        \"employee_recog\":employee_recog\n",
    "    }    \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "    page_titles = pages.keys()\n",
    "    page_title = st.sidebar.selectbox(\n",
    "        \"Choose the app mode\",\n",
    "        page_titles,\n",
    "    )\n",
    "    st.subheader(page_title)\n",
    "    page_func = pages[page_title]\n",
    "    page_func()\n",
    "    st.sidebar.markdown(\n",
    "        \"\"\"\n",
    "---\n",
    "    \"\"\",  # noqa: E501\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "    logger.debug(\"=== Alive threads ===\")\n",
    "    for thread in threading.enumerate():\n",
    "        if thread.is_alive():\n",
    "            logger.debug(f\"  {thread.name} ({thread.ident})\")\n",
    "   \n",
    "def app_loopback():\n",
    "    \"\"\"Simple video loopback\"\"\"\n",
    "    webrtc_streamer(key=\"loopback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551cfc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emprec(img):    \n",
    "   \n",
    "    facesInFrame = face_rec.face_locations(img)\n",
    "    if len(facesInFrame) > 0:\n",
    "        encodeFacesInFrame = face_rec.face_encodings(img, facesInFrame)\n",
    "        for encodeFace, faceloc in zip(encodeFacesInFrame, facesInFrame) :\n",
    "            matches = face_rec.compare_faces(EncodeList, encodeFace)\n",
    "            facedis = face_rec.face_distance(EncodeList, encodeFace)\n",
    "            print(facedis)\n",
    "            if min(facedis) < 0.5:\n",
    "                matchIndex = np.argmin(facedis)\n",
    "            print(matchIndex)\n",
    "            name = employeeName[matchIndex].upper()\n",
    "#             y1, x2, y2, x1 = faceloc\n",
    "#             y1, x2, y2, x1 = y1*4, x2*4, y2*4, x1*4\n",
    "#             cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "#             cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), cv2.FILLED)\n",
    "#             cv2.putText(img, name, (x1+6, y2-6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "            top, right, bottom, left = faceloc\n",
    "            cv2.rectangle(img, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "            cv2.putText(img, name,  (left + 6, bottom - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 1)\n",
    "            print(name)        \n",
    "            MarkAttendence(name)\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567dc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_video_filters():\n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "#    _type = st.radio(\"Select transform type\", (\"noop\", \"cartoon\", \"edges\", \"rotate\"))\n",
    "    def callback(frame: av.VideoFrame) -> av.VideoFrame:\n",
    "        img = frame.to_ndarray(format=\"bgr24\")\n",
    "        #img = emprec(img)\n",
    "       \n",
    "       \n",
    "               \n",
    "       \n",
    "       \n",
    "#         if _type == \"noop\":\n",
    "#             pass\n",
    "#         elif _type == \"cartoon\":\n",
    "#             # prepare color\n",
    "#             img_color = cv2.pyrDown(cv2.pyrDown(img))\n",
    "#             for _ in range(6):\n",
    "#                 img_color = cv2.bilateralFilter(img_color, 9, 9, 7)\n",
    "#             img_color = cv2.pyrUp(cv2.pyrUp(img_color))\n",
    "#             # prepare edges\n",
    "#             img_edges = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "#             img_edges = cv2.adaptiveThreshold(\n",
    "#                 cv2.medianBlur(img_edges, 7),\n",
    "#                 255,\n",
    "#                 cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "#                 cv2.THRESH_BINARY,\n",
    "#                 9,\n",
    "#                 2,\n",
    "#             )\n",
    "#             img_edges = cv2.cvtColor(img_edges, cv2.COLOR_GRAY2RGB)\n",
    "#             # combine color and edges\n",
    "#             img = cv2.bitwise_and(img_color, img_edges)\n",
    "#         elif _type == \"edges\":\n",
    "#             # perform edge detection\n",
    "#             img = cv2.cvtColor(cv2.Canny(img, 100, 200), cv2.COLOR_GRAY2BGR)\n",
    "#         elif _type == \"rotate\":\n",
    "#             # rotate image\n",
    "#             rows, cols, _ = img.shape\n",
    "#             M = cv2.getRotationMatrix2D((cols / 2, rows / 2), frame.time * 45, 1)\n",
    "#             img = cv2.warpAffine(img, M, (cols, rows))\n",
    "        return av.VideoFrame.from_ndarray(img, format=\"bgr24\")\n",
    "    webrtc_streamer(\n",
    "        key=\"opencv-filter\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        video_frame_callback=callback,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        async_processing=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971fdc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_audio_filter():\n",
    "    gain = st.slider(\"Gain\", -10.0, +20.0, 1.0, 0.05)\n",
    "    def process_audio(frame: av.AudioFrame) -> av.AudioFrame:\n",
    "        raw_samples = frame.to_ndarray()\n",
    "        sound = pydub.AudioSegment(\n",
    "            data=raw_samples.tobytes(),\n",
    "            sample_width=frame.format.bytes,\n",
    "            frame_rate=frame.sample_rate,\n",
    "            channels=len(frame.layout.channels),\n",
    "        )\n",
    "        sound = sound.apply_gain(gain)\n",
    "        # Ref: https://github.com/jiaaro/pydub/blob/master/API.markdown#audiosegmentget_array_of_samples  # noqa\n",
    "        channel_sounds = sound.split_to_mono()\n",
    "        channel_samples = [s.get_array_of_samples() for s in channel_sounds]\n",
    "        new_samples: np.ndarray = np.array(channel_samples).T\n",
    "        new_samples = new_samples.reshape(raw_samples.shape)\n",
    "        new_frame = av.AudioFrame.from_ndarray(new_samples, layout=frame.layout.name)\n",
    "        new_frame.sample_rate = frame.sample_rate\n",
    "        return new_frame\n",
    "    webrtc_streamer(\n",
    "        key=\"audio-filter\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        audio_frame_callback=process_audio,\n",
    "        async_processing=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_delayed_echo():\n",
    "    delay = st.slider(\"Delay\", 0.0, 5.0, 1.0, 0.05)\n",
    "    async def queued_video_frames_callback(\n",
    "        frames: List[av.VideoFrame],\n",
    "    ) -> List[av.VideoFrame]:\n",
    "        logger.debug(\"Delay: %f\", delay)\n",
    "        # A standalone `await ...` is interpreted as an expression and\n",
    "        # the Streamlit magic's target, which leads implicit calls of `st.write`.\n",
    "        # To prevent it, fix it as `_ = await ...`, a statement.\n",
    "        # See https://discuss.streamlit.io/t/issue-with-asyncio-run-in-streamlit/7745/15\n",
    "        _ = await asyncio.sleep(delay)\n",
    "        return frames\n",
    "    async def queued_audio_frames_callback(\n",
    "        frames: List[av.AudioFrame],\n",
    "    ) -> List[av.AudioFrame]:\n",
    "        _ = await asyncio.sleep(delay)\n",
    "        return frames\n",
    "    webrtc_streamer(\n",
    "        key=\"delay\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        queued_video_frames_callback=queued_video_frames_callback,\n",
    "        queued_audio_frames_callback=queued_audio_frames_callback,\n",
    "        async_processing=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b53af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_object_detection():\n",
    "    MODEL_URL = \"https://github.com/robmarkcole/object-detection-app/raw/master/model/MobileNetSSD_deploy.caffemodel\"  # noqa: E501\n",
    "    MODEL_LOCAL_PATH = HERE / \"./models/MobileNetSSD_deploy.caffemodel\"\n",
    "    PROTOTXT_URL = \"https://github.com/robmarkcole/object-detection-app/raw/master/model/MobileNetSSD_deploy.prototxt.txt\"  # noqa: E501\n",
    "    PROTOTXT_LOCAL_PATH = HERE / \"./models/MobileNetSSD_deploy.prototxt.txt\"\n",
    "    CLASSES = [\n",
    "        \"background\",\n",
    "        \"aeroplane\",\n",
    "        \"bicycle\",\n",
    "        \"bird\",\n",
    "        \"boat\",\n",
    "        \"bottle\",\n",
    "        \"bus\",\n",
    "        \"car\",\n",
    "        \"cat\",\n",
    "        \"chair\",\n",
    "        \"cow\",\n",
    "        \"diningtable\",\n",
    "        \"dog\",\n",
    "        \"horse\",\n",
    "        \"motorbike\",\n",
    "        \"person\",\n",
    "        \"pottedplant\",\n",
    "        \"sheep\",\n",
    "        \"sofa\",\n",
    "        \"train\",\n",
    "        \"tvmonitor\",\n",
    "    ]\n",
    "    @st.experimental_singleton\n",
    "    def generate_label_colors():\n",
    "        return np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "    COLORS = generate_label_colors()\n",
    "    download_file(MODEL_URL, MODEL_LOCAL_PATH, expected_size=23147564)\n",
    "    download_file(PROTOTXT_URL, PROTOTXT_LOCAL_PATH, expected_size=29353)\n",
    "    DEFAULT_CONFIDENCE_THRESHOLD = 0.5\n",
    "    class Detection(NamedTuple):\n",
    "        name: str\n",
    "        prob: float\n",
    "    # Session-specific caching\n",
    "    cache_key = \"object_detection_dnn\"\n",
    "    if cache_key in st.session_state:\n",
    "        net = st.session_state[cache_key]\n",
    "    else:\n",
    "        net = cv2.dnn.readNetFromCaffe(str(PROTOTXT_LOCAL_PATH), str(MODEL_LOCAL_PATH))\n",
    "        st.session_state[cache_key] = net\n",
    "    confidence_threshold = st.slider(\n",
    "        \"Confidence threshold\", 0.0, 1.0, DEFAULT_CONFIDENCE_THRESHOLD, 0.05\n",
    "    )\n",
    "    def _annotate_image(image, detections):\n",
    "        # loop over the detections\n",
    "        (h, w) = image.shape[:2]\n",
    "        result: List[Detection] = []\n",
    "        for i in np.arange(0, detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > confidence_threshold:\n",
    "                # extract the index of the class label from the `detections`,\n",
    "                # then compute the (x, y)-coordinates of the bounding box for\n",
    "                # the object\n",
    "                idx = int(detections[0, 0, i, 1])\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                name = CLASSES[idx]\n",
    "                result.append(Detection(name=name, prob=float(confidence)))\n",
    "                # display the prediction\n",
    "                label = f\"{name}: {round(confidence * 100, 2)}%\"\n",
    "                cv2.rectangle(image, (startX, startY), (endX, endY), COLORS[idx], 2)\n",
    "                y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    label,\n",
    "                    (startX, y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    COLORS[idx],\n",
    "                    2,\n",
    "                )\n",
    "        return image, result\n",
    "    result_queue = (\n",
    "        queue.Queue()\n",
    "    )  # TODO: A general-purpose shared state object may be more useful.\n",
    "    def callback(frame: av.VideoFrame) -> av.VideoFrame:\n",
    "        image = frame.to_ndarray(format=\"bgr24\")\n",
    "        blob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(image, (300, 300)), 0.007843, (300, 300), 127.5\n",
    "        )\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        annotated_image, result = _annotate_image(image, detections)\n",
    "        # NOTE: This `recv` method is called in another thread,\n",
    "        # so it must be thread-safe.\n",
    "        result_queue.put(result)  # TODO:\n",
    "        return av.VideoFrame.from_ndarray(annotated_image, format=\"bgr24\")\n",
    "    webrtc_ctx = webrtc_streamer(\n",
    "        key=\"object-detection\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        video_frame_callback=callback,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        async_processing=True,\n",
    "    )\n",
    "    if st.checkbox(\"Show the detected labels\", value=True):\n",
    "        if webrtc_ctx.state.playing:\n",
    "            labels_placeholder = st.empty()\n",
    "            # NOTE: The video transformation with object detection and\n",
    "            # this loop displaying the result labels are running\n",
    "            # in different threads asynchronously.\n",
    "            # Then the rendered video frames and the labels displayed here\n",
    "            # are not strictly synchronized.\n",
    "            while True:\n",
    "                try:\n",
    "                    result = result_queue.get(timeout=1.0)\n",
    "                except queue.Empty:\n",
    "                    result = None\n",
    "                labels_placeholder.table(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_streaming():\n",
    "    \"\"\"Media streamings\"\"\"\n",
    "    MEDIAFILES = {\n",
    "        \"big_buck_bunny_720p_2mb.mp4 (local)\": {\n",
    "            \"url\": \"https://sample-videos.com/video123/mp4/720/big_buck_bunny_720p_2mb.mp4\",  # noqa: E501\n",
    "            \"local_file_path\": HERE / \"data/big_buck_bunny_720p_2mb.mp4\",\n",
    "            \"type\": \"video\",\n",
    "        },\n",
    "        \"big_buck_bunny_720p_10mb.mp4 (local)\": {\n",
    "            \"url\": \"https://sample-videos.com/video123/mp4/720/big_buck_bunny_720p_10mb.mp4\",  # noqa: E501\n",
    "            \"local_file_path\": HERE / \"data/big_buck_bunny_720p_10mb.mp4\",\n",
    "            \"type\": \"video\",\n",
    "        },\n",
    "        \"file_example_MP3_700KB.mp3 (local)\": {\n",
    "            \"url\": \"https://file-examples-com.github.io/uploads/2017/11/file_example_MP3_700KB.mp3\",  # noqa: E501\n",
    "            \"local_file_path\": HERE / \"data/file_example_MP3_700KB.mp3\",\n",
    "            \"type\": \"audio\",\n",
    "        },\n",
    "        \"file_example_MP3_5MG.mp3 (local)\": {\n",
    "            \"url\": \"https://file-examples-com.github.io/uploads/2017/11/file_example_MP3_5MG.mp3\",  # noqa: E501\n",
    "            \"local_file_path\": HERE / \"data/file_example_MP3_5MG.mp3\",\n",
    "            \"type\": \"audio\",\n",
    "        },\n",
    "        \"rtsp://wowzaec2demo.streamlock.net/vod/mp4:BigBuckBunny_115k.mov\": {\n",
    "            \"url\": \"rtsp://wowzaec2demo.streamlock.net/vod/mp4:BigBuckBunny_115k.mov\",\n",
    "            \"type\": \"video\",\n",
    "        },\n",
    "    }\n",
    "    media_file_label = st.radio(\n",
    "        \"Select a media source to stream\", tuple(MEDIAFILES.keys())\n",
    "    )\n",
    "    media_file_info = MEDIAFILES[media_file_label]\n",
    "    if \"local_file_path\" in media_file_info:\n",
    "        download_file(media_file_info[\"url\"], media_file_info[\"local_file_path\"])\n",
    "    def create_player():\n",
    "        if \"local_file_path\" in media_file_info:\n",
    "            return MediaPlayer(str(media_file_info[\"local_file_path\"]))\n",
    "        else:\n",
    "            return MediaPlayer(media_file_info[\"url\"])\n",
    "        # NOTE: To stream the video from webcam, use the code below.\n",
    "        # return MediaPlayer(\n",
    "        #     \"1:none\",\n",
    "        #     format=\"avfoundation\",\n",
    "        #     options={\"framerate\": \"30\", \"video_size\": \"1280x720\"},\n",
    "        # )\n",
    "    key = f\"media-streaming-{media_file_label}\"\n",
    "    ctx: Optional[WebRtcStreamerContext] = st.session_state.get(key)\n",
    "    if media_file_info[\"type\"] == \"video\" and ctx and ctx.state.playing:\n",
    "        _type = st.radio(\n",
    "            \"Select transform type\", (\"noop\", \"cartoon\", \"edges\", \"rotate\")\n",
    "        )\n",
    "    else:\n",
    "        _type = \"noop\"\n",
    "    def video_frame_callback(frame: av.VideoFrame) -> av.VideoFrame:\n",
    "        img = frame.to_ndarray(format=\"bgr24\")\n",
    "        if _type == \"noop\":\n",
    "            pass\n",
    "        elif _type == \"cartoon\":\n",
    "            # prepare color\n",
    "            img_color = cv2.pyrDown(cv2.pyrDown(img))\n",
    "            for _ in range(6):\n",
    "                img_color = cv2.bilateralFilter(img_color, 9, 9, 7)\n",
    "            img_color = cv2.pyrUp(cv2.pyrUp(img_color))\n",
    "            # prepare edges\n",
    "            img_edges = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            img_edges = cv2.adaptiveThreshold(\n",
    "                cv2.medianBlur(img_edges, 7),\n",
    "                255,\n",
    "                cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                cv2.THRESH_BINARY,\n",
    "                9,\n",
    "                2,\n",
    "            )\n",
    "            img_edges = cv2.cvtColor(img_edges, cv2.COLOR_GRAY2RGB)\n",
    "            # combine color and edges\n",
    "            img = cv2.bitwise_and(img_color, img_edges)\n",
    "        elif _type == \"edges\":\n",
    "            # perform edge detection\n",
    "            img = cv2.cvtColor(cv2.Canny(img, 100, 200), cv2.COLOR_GRAY2BGR)\n",
    "        elif _type == \"rotate\":\n",
    "            # rotate image\n",
    "            rows, cols, _ = img.shape\n",
    "            M = cv2.getRotationMatrix2D((cols / 2, rows / 2), frame.time * 45, 1)\n",
    "            img = cv2.warpAffine(img, M, (cols, rows))\n",
    "        return av.VideoFrame.from_ndarray(img, format=\"bgr24\")\n",
    "    webrtc_streamer(\n",
    "        key=key,\n",
    "        mode=WebRtcMode.RECVONLY,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\n",
    "            \"video\": media_file_info[\"type\"] == \"video\",\n",
    "            \"audio\": media_file_info[\"type\"] == \"audio\",\n",
    "        },\n",
    "        player_factory=create_player,\n",
    "        video_frame_callback=video_frame_callback,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a57193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_sendonly_video():\n",
    "    \"\"\"A sample to use WebRTC in sendonly mode to transfer frames\n",
    "    from the browser to the server and to render frames via `st.image`.\"\"\"\n",
    "    webrtc_ctx = webrtc_streamer(\n",
    "        key=\"video-sendonly\",\n",
    "        mode=WebRtcMode.SENDONLY,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True},\n",
    "    )\n",
    "    image_place = st.empty()\n",
    "    while True:\n",
    "        if webrtc_ctx.video_receiver:\n",
    "            try:\n",
    "                video_frame = webrtc_ctx.video_receiver.get_frame(timeout=1)\n",
    "            except queue.Empty:\n",
    "                logger.warning(\"Queue is empty. Abort.\")\n",
    "                break\n",
    "            img_rgb = video_frame.to_ndarray(format=\"rgb24\")\n",
    "            image_place.image(img_rgb)\n",
    "        else:\n",
    "            logger.warning(\"AudioReciver is not set. Abort.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e70b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_sendonly_audio():\n",
    "    \"\"\"A sample to use WebRTC in sendonly mode to transfer audio frames\n",
    "    from the browser to the server and visualize them with matplotlib\n",
    "    and `st.pyplot`.\"\"\"\n",
    "    webrtc_ctx = webrtc_streamer(\n",
    "        key=\"sendonly-audio\",\n",
    "        mode=WebRtcMode.SENDONLY,\n",
    "        audio_receiver_size=256,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"audio\": True},\n",
    "    )\n",
    "    fig_place = st.empty()\n",
    "    fig, [ax_time, ax_freq] = plt.subplots(\n",
    "        2, 1, gridspec_kw={\"top\": 1.5, \"bottom\": 0.2}\n",
    "    )\n",
    "    sound_window_len = 5000  # 5s\n",
    "    sound_window_buffer = None\n",
    "    while True:\n",
    "        if webrtc_ctx.audio_receiver:\n",
    "            try:\n",
    "                audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=1)\n",
    "            except queue.Empty:\n",
    "                logger.warning(\"Queue is empty. Abort.\")\n",
    "                break\n",
    "            sound_chunk = pydub.AudioSegment.empty()\n",
    "            for audio_frame in audio_frames:\n",
    "                sound = pydub.AudioSegment(\n",
    "                    data=audio_frame.to_ndarray().tobytes(),\n",
    "                    sample_width=audio_frame.format.bytes,\n",
    "                    frame_rate=audio_frame.sample_rate,\n",
    "                    channels=len(audio_frame.layout.channels),\n",
    "                )\n",
    "                sound_chunk += sound\n",
    "            if len(sound_chunk) > 0:\n",
    "                if sound_window_buffer is None:\n",
    "                    sound_window_buffer = pydub.AudioSegment.silent(\n",
    "                        duration=sound_window_len\n",
    "                    )\n",
    "                sound_window_buffer += sound_chunk\n",
    "                if len(sound_window_buffer) > sound_window_len:\n",
    "                    sound_window_buffer = sound_window_buffer[-sound_window_len:]\n",
    "            if sound_window_buffer:\n",
    "                # Ref: https://own-search-and-study.xyz/2017/10/27/python%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E9%9F%B3%E5%A3%B0%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8B%E3%82%89%E3%82%B9%E3%83%9A%E3%82%AF%E3%83%88%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0%E3%82%92%E4%BD%9C/  # noqa\n",
    "                sound_window_buffer = sound_window_buffer.set_channels(\n",
    "                    1\n",
    "                )  # Stereo to mono\n",
    "                sample = np.array(sound_window_buffer.get_array_of_samples())\n",
    "                ax_time.cla()\n",
    "                times = (np.arange(-len(sample), 0)) / sound_window_buffer.frame_rate\n",
    "                ax_time.plot(times, sample)\n",
    "                ax_time.set_xlabel(\"Time\")\n",
    "                ax_time.set_ylabel(\"Magnitude\")\n",
    "                spec = np.fft.fft(sample)\n",
    "                freq = np.fft.fftfreq(sample.shape[0], 1.0 / sound_chunk.frame_rate)\n",
    "                freq = freq[: int(freq.shape[0] / 2)]\n",
    "                spec = spec[: int(spec.shape[0] / 2)]\n",
    "                spec[0] = spec[0] / 2\n",
    "                ax_freq.cla()\n",
    "                ax_freq.plot(freq, np.abs(spec))\n",
    "                ax_freq.set_xlabel(\"Frequency\")\n",
    "                ax_freq.set_yscale(\"log\")\n",
    "                ax_freq.set_ylabel(\"Magnitude\")\n",
    "                fig_place.pyplot(fig)\n",
    "        else:\n",
    "            logger.warning(\"AudioReciver is not set. Abort.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e1a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_media_constraints():\n",
    "    \"\"\"A sample to configure MediaStreamConstraints object\"\"\"\n",
    "    frame_rate = 5\n",
    "    webrtc_streamer(\n",
    "        key=\"media-constraints\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\n",
    "            \"video\": {\"frameRate\": {\"ideal\": frame_rate}},\n",
    "        },\n",
    "        video_html_attrs={\n",
    "            \"style\": {\"width\": \"50%\", \"margin\": \"0 auto\", \"border\": \"5px yellow solid\"},\n",
    "            \"controls\": False,\n",
    "            \"autoPlay\": True,\n",
    "        },\n",
    "    )\n",
    "    st.write(f\"The frame rate is set as {frame_rate}. Video style is changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_programatically_play():\n",
    "    \"\"\"A sample of controlling the playing state from Python.\"\"\"\n",
    "    playing = st.checkbox(\"Playing\", value=True)\n",
    "    webrtc_streamer(\n",
    "        key=\"programatic_control\",\n",
    "        desired_playing_state=playing,\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5946b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_customize_ui_texts():\n",
    "    webrtc_streamer(\n",
    "        key=\"custom_ui_texts\",\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        translations={\n",
    "            \"start\": \"開始\",\n",
    "            \"stop\": \"停止\",\n",
    "            \"select_device\": \"デバイス選択\",\n",
    "            \"media_api_not_available\": \"Media APIが利用できない環境です\",\n",
    "            \"device_ask_permission\": \"メディアデバイスへのアクセスを許可してください\",\n",
    "            \"device_not_available\": \"メディアデバイスを利用できません\",\n",
    "            \"device_access_denied\": \"メディアデバイスへのアクセスが拒否されました\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "if st.button('show attendance'):\n",
    "    st.dataframe(pd.read_csv('attendance.csv'))\n",
    "                 \n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    DEBUG = os.environ.get(\"DEBUG\", \"false\").lower() not in [\"false\", \"no\", \"0\"]\n",
    "    logging.basicConfig(\n",
    "        format=\"[%(asctime)s] %(levelname)7s from %(name)s in %(pathname)s:%(lineno)d: \"\n",
    "        \"%(message)s\",\n",
    "        force=True,\n",
    "    )\n",
    "    logger.setLevel(level=logging.DEBUG if DEBUG else logging.INFO)\n",
    "    st_webrtc_logger = logging.getLogger(\"streamlit_webrtc\")\n",
    "    st_webrtc_logger.setLevel(logging.DEBUG)\n",
    "    fsevents_logger = logging.getLogger(\"fsevents\")\n",
    "    fsevents_logger.setLevel(logging.WARNING)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ef990",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('video', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd362e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
